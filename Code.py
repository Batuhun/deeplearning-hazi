# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12waKp9VgedzEBSqHk2Ukeni4H0fEUNZy

# Install torch
"""

import torch
import os
import urllib.request
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool  # Global pooling
from sklearn.model_selection import train_test_split
from torch_geometric.nn import BatchNorm
from node2vec import Node2Vec
from sklearn.model_selection import train_test_split
import networkx as nx
import matplotlib.pylab as plt
import seaborn as sns
import numpy as np
import gzip
import zipfile
from torch_geometric.data import DataLoader

import torch

"""# Download the `facebook_combined.txt.gz` dataset
## From: https://snap.stanford.edu/data/ego-Facebook.html
"""

# Step 1: Download the Facebook Ego dataset
url = "https://snap.stanford.edu/data/facebook_combined.txt.gz"
dataset_path = "facebook_combined.txt.gz"

if not os.path.exists(dataset_path):
    urllib.request.urlretrieve(url, dataset_path)
    print("Facebook dataset downloaded.")

"""
# Data analysis"""

# Load the original graph from the edge list
graph = nx.read_edgelist('facebook_combined.txt.gz', delimiter=' ', create_using=nx.Graph(), nodetype=int)

# Get the first 50 nodes
subgraph = list(graph.nodes())[25:75]

# Create a subgraph with only those 50 nodes
subgraph = graph.subgraph(subgraph)

# Generate the layout for the subgraph
pos = pos = nx.circular_layout(subgraph)
# Draw the subgraph
nx.draw(subgraph, pos, node_color='#A0CBE2', edge_color='#808080', width=1, edge_cmap=plt.cm.Blues, with_labels=True, arrows=False)

# Show the plot
plt.show()

print("The number of unique persons",len(graph.nodes()))

degree_dist = list(dict(graph.degree()).values())
degree_dist.sort()
plt.figure(figsize=(10,6))
plt.plot(degree_dist)
plt.xlabel('Index No')
plt.ylabel('No Of Followers')
plt.show()

# let's zoom in
degree_dist = list(dict(graph.degree()).values())
degree_dist.sort()
plt.figure(figsize=(10,6))
plt.plot(degree_dist[0:2000])
plt.xlabel('Index No')
plt.ylabel('No Of Followers')
plt.show()

plt.boxplot(degree_dist)
plt.ylabel('No of followers')
plt.show()

# 0 and 90-100 percentile
print(0, "percentile value is", np.percentile(degree_dist,0))
for i in range(0,11):
  print(90+i, "percentile value is", np.percentile(degree_dist,90+i))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sns.set_style('ticks')
fig, ax = plt.subplots()
sns.histplot(degree_dist, color='#16A085')
plt.xlabel('PDF of Degree')
sns.despine()
plt.show()

"""# Data preprocessing"""

# Step 2: Preprocess the data - Create edge list

edges = []
with gzip.open(dataset_path, 'rt') as f:
    for line in f:
        src, dst = map(int, line.strip().split())
        edges.append([src, dst])
edges_list = edges
from collections import Counter
degrees = Counter([edge[0] for edge in edges_list] + [edge[1] for edge in edges_list])

edges_list = edges
print(len(edges_list))
edges_list = [edge for edge in edges_list if edge[0] != edge[1]]
edges_list = list(set([tuple(sorted(edge)) for edge in edges_list]))
# remove nodes (and their edges) that have a degree below 2
edges_list = [edge for edge in edges_list if degrees[edge[0]] > 1 and degrees[edge[1]] > 1]
print(len(edges_list))
# remove top 1% of nodes with highest degrees
high_degree_threshold = 0.99
threshold = np.percentile(list(degrees.values()), 99)
edges_list = [edge for edge in edges_list if degrees[edge[0]] < threshold and degrees[edge[1]] < threshold]
print(len(edges_list))

"""# Adding extra features to the nodes"""

# Check if GPU is available and set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


edges_copy = edges_list
torch_edges = torch.tensor(edges_copy, dtype=torch.long).t().contiguous()
# Number of nodes
num_nodes = torch_edges.max().item() + 1

# Create node features based on node degree
node_degrees = torch.tensor([degrees[node] for node in range(num_nodes)], dtype=torch.float)

# Reshape node features to have them as a feature vector (degree as a single feature)
x = node_degrees.view(-1, 1)

# Calculate the average clustering coefficient
avg_clustering_coeff = nx.average_clustering(graph)

# Add the average clustering coefficient as a feature for every node
avg_clustering_tensor = torch.full((num_nodes, 1), avg_clustering_coeff, dtype=torch.float)

# Append the feature to the existing node features
x = torch.cat([x, avg_clustering_tensor], dim=1)

# Add the diameter as a feature for each node
graph_diameter = 8  # based on the dataset information
diameter_tensor = torch.full((num_nodes, 1), graph_diameter, dtype=torch.float)

# Add the diameter as a feature to each node
x = torch.cat([x, diameter_tensor], dim=1)


# Initialize the Node2Vec model
node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=1)

# Fit the model to generate embeddings
model = node2vec.fit(window=5, min_count=1)

# Get embeddings for all nodes
embeddings = [model.wv[str(node)] for node in range(num_nodes)]

# Convert embeddings to a tensor
embedding_tensor = torch.tensor(embeddings, dtype=torch.float)

# Concatenate the embeddings to the existing feature matrix
x = torch.cat([x, embedding_tensor], dim=1)

# Update the Data object with the new feature matrix
data = Data(x=x, edge_index=torch_edges)

print(f"Node features based on degree: {data.x[:5]}")

"""#Generating negative edges and splitting up data

"""

# Step 5: Create labels for friend recommendation
# Positive samples (existing edges)
positive_edges = torch.tensor(edges_copy, dtype=torch.long).t().contiguous()

# Generate negative samples
num_nodes = torch_edges.max().item() + 1
negative_edges = []
while len(negative_edges) < len(positive_edges[0]) // 2:  # Half the size of positive edges
    src = torch.randint(0, num_nodes, (1,)).item()
    dst = torch.randint(0, num_nodes, (1,)).item()
    if src != dst and (src, dst) not in edges_list and (dst, src) not in edges_list:
        negative_edges.append([src, dst])

all_edges = torch.cat([positive_edges, torch.tensor(negative_edges, dtype=torch.long).t()], dim=1)
all_labels = torch.cat([torch.ones(positive_edges.size(1)), torch.zeros(len(negative_edges))])  # Labels for edges

# Step 1: Split into train and combined test/validation set
train_edges, test_val_edges, train_labels, test_val_labels = train_test_split(
    all_edges.t(), all_labels, test_size=0.2, random_state=42
)

# Create Data object for the training set
train_data = Data(x=x, edge_index=train_edges.t().contiguous(), y=train_labels)

# Step 2: Split the combined test/validation set into test and validation sets
val_edges, test_edges, val_labels, test_labels = train_test_split(
    test_val_edges, test_val_labels, test_size=0.5, random_state=42
)

# Create Data objects for validation and test sets
val_data = Data(x=x, edge_index=val_edges.t().contiguous(), y=val_labels)
test_data = Data(x=x, edge_index=test_edges.t().contiguous(), y=test_labels)

"""#Training and testing"""

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

class GCNEdgePrediction(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, dropout=0.25):
        super(GCNEdgePrediction, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.fc1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)  # For concatenation
        self.fc2 = torch.nn.Linear(hidden_channels, 1)  # Output single score for each edge
        self.dropout = dropout

    def forward(self, x, edge_index, edge_label_index):
        # GCN Layers
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.relu(x)

        # Edge Embeddings
        src, dst = edge_label_index  # Edge indices
        edge_embeddings = torch.cat([x[src], x[dst]], dim=1)  # Concatenate source and target embeddings
        edge_scores = self.fc2(F.relu(self.fc1(edge_embeddings)))
        return edge_scores

# Initialize the model and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Check and set device
model = GCNEdgePrediction(in_channels=data.x.size(1), hidden_channels=128, dropout=0.5).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)

# Ensure data and labels are on the same device
train_data = train_data.to(device)
val_data = val_data.to(device)
test_data = test_data.to(device)

# Training and testing functions
def train():
    model.train()
    optimizer.zero_grad()
    out = model(train_data.x, train_data.edge_index, train_data.edge_index)  # Use train_data.edge_index
    loss = F.binary_cross_entropy_with_logits(out, train_data.y.float().unsqueeze(1))  # Ensure correct shape
    loss.backward()
    optimizer.step()
    return loss.item()

from sklearn.metrics import accuracy_score, roc_auc_score

def test(data):
    model.eval()
    with torch.no_grad():
        # Get predictions as probabilities
        out = model(data.x, data.edge_index, data.edge_index)
        probs = torch.sigmoid(out).squeeze()  # Apply sigmoid for binary classification probabilities

        # Classify based on a threshold of 0.5
        pred_labels = (probs > 0.5).float()

        # Calculate accuracy
        acc = accuracy_score(data.y.cpu(), pred_labels.cpu())

        # Optionally, calculate ROC-AUC for more insight
        auc = roc_auc_score(data.y.cpu(), probs.cpu())

        return acc, auc

def validate():
    # Evaluate on the validation set
    val_acc, val_auc = test(val_data)  # Reuse the `test` function with `val_data`
    return val_acc, val_auc




patience = 100  # Early stopping patience
best_val_auc = 0.0
epochs_without_improvement = 0

for epoch in range(1, 1001):
    # Training step
    train_loss = train()

    # Validation step to monitor performance
    val_acc, val_auc = validate()

    # Early stopping based on validation AUC
    if val_auc > best_val_auc:
        best_val_auc = val_auc
        epochs_without_improvement = 0
        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model based on validation AUC
    else:
        epochs_without_improvement += 1

    # Stop training if no improvement within patience epochs
    if epochs_without_improvement >= patience:
        print(f"Early stopping at epoch {epoch}, best validation AUC: {best_val_auc:.4f}")
        break

    # Optionally print progress every 20 epochs
    if epoch % 20 == 0:
        print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val AUC: {val_auc:.4f}")

# Final test evaluation after training completes
model.load_state_dict(torch.load('best_model.pth'))  # Load the best model based on validation AUC
test_acc, test_auc = test(test_data)
print(f"Final Test Accuracy: {test_acc:.4f}, Test AUC: {test_auc:.4f}")

"""#Hyper parameter optimization? tbd"""

import optuna
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from sklearn.metrics import roc_auc_score
from torch_geometric.data import DataLoader

# Define your model class with adjustable hyperparameters
class GCNEdgePrediction(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, num_layers, dropout):
        super(GCNEdgePrediction, self).__init__()
        self.layers = torch.nn.ModuleList()
        self.layers.append(GCNConv(in_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.layers.append(GCNConv(hidden_channels, hidden_channels))
        self.fc1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)
        self.fc2 = torch.nn.Linear(hidden_channels, 1)
        self.dropout = dropout

    def forward(self, x, edge_index, edge_label_index):
        # GCN Layers
        for layer in self.layers:
            x = layer(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # Edge Embeddings
        src, dst = edge_label_index
        edge_embeddings = torch.cat([x[src], x[dst]], dim=1)
        edge_scores = self.fc2(F.relu(self.fc1(edge_embeddings)))
        return edge_scores

# Objective function for Optuna
def objective(trial):
    # Suggest hyperparameters to tune
    hidden_channels = trial.suggest_int('hidden_channels', 64, 256)
    num_layers = trial.suggest_int('num_layers', 2, 4)
    dropout = trial.suggest_float('dropout', 0.2, 0.5)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)

    # Initialize the model and optimizer with trial-suggested parameters
    model = GCNEdgePrediction(
        in_channels=data.x.size(1),
        hidden_channels=hidden_channels,
        num_layers=num_layers,
        dropout=dropout
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    for epoch in range(50):  # A small number of epochs to save time
        model.train()
        optimizer.zero_grad()
        out = model(train_data.x, train_data.edge_index, train_data.edge_index)
        loss = F.binary_cross_entropy_with_logits(out, train_data.y.float().unsqueeze(1).to(device))
        loss.backward()
        optimizer.step()

    # Validation step
    val_acc, val_auc = test(val_data)  # Assume `test` function works with validation data
    return val_auc  # Return validation AUC as the metric to maximize

# Running Optuna optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Best trial results
print("Best trial:")
trial = study.best_trial
print(f"  AUC: {trial.value}")
print("  Best hyperparameters:", trial.params)

